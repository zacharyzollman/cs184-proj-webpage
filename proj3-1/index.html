<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>CS 184 Projects</title>
    <link rel="stylesheet" href="./style.css">
    <link rel="icon" href="./icon.png" type="image/x-icon">
    <style>
      h1 {text-align: center;}
      h5 {text-align: center;}
	  table, th, td { border: 1px solid black;}
	  th, td { padding: 10px; text-align: center;}
      </style>
  </head>
  
  <body style="margin: 5%;">
    <main>

    <h1>Project 3-1: Pathtracer</h1>
    <h5>Abdul Ali Khan, Zachary Zollman</h5>
	
    
    <h2>Overview</h2>
		This project is focused on implementing basic ray tracing, which is a rendering technique that simulates the behavior of light in a scene by tracing the path of light rays as they bounce around and interact with objects in the scene. To simplify things, our basic path tracing only works with diffuse objects. The project is divided into five parts, each focusing on a different aspect of our Path Tracer.
		<br><br>
		In Part 1, we implemented the basic ray generation and primitive intersection parts of the rendering pipeline. Below, we explain the triangle intersection algorithm used to determine if a ray intersects a triangle in the scene. In Part 2, we implemented a Bounding Volume Hierarchy (BVH) construction algorithm to speed up rendering times for scenes with complex geometry. We explain the heuristic used to pick the splitting point and compare rendering times with and without BVH acceleration below. In Part 3, we implemented both uniform hemisphere sampling and light sampling for the direct lighting function. We then compared noise levels in soft shadows for different numbers of light rays and samples per pixel. In Part 4, we implemented the indirect lighting function and compared rendered views with only direct illumination and only indirect illumination. We also compared views with different maximum ray depths and sample-per-pixel rates. In Part 5, we implemented adaptive sampling and rendered two scenes with clearly visible differences in sampling rates over various regions and pixels.
		<br><br>
		Overall, this project provided an in-depth understanding of various concepts related to the Path Tracer, including ray tracing, Bounding Volume Hierarchies, direct and indirect lighting functions, and adaptive sampling. The project allowed us to implement these concepts in code and compare the results with different settings and parameters. We also gained a better understanding of the importance of optimization techniques to achieve faster rendering times for complex scenes.	
	<br></br>
	
	<h2>Part 1: Ray Generation and Scene Intersection</h2>	
		<h3>Walk through the ray generation and primitive intersection parts of the rendering pipeline.</h3>
			The rendering pipeline starts with the generation of a camera ray for each pixel in the image. For each pixel, the <code>raytrace_pixel</code> function is called, which generates <code>ns_aa</code> camera rays and traces them through the scene. The <code>generate_ray</code> function of the <code>Camera</code> class computes a normalized ray in world space given input coordinates in image space. Thus, 2D pixel coordinates are used to determine 3D ray directions. The ray direction is computed by converting coordinates from image space to camera space, then creating a ray in camera space, and then transforming the ray from camera space to world space.
			<br><br>
			In the <code>raytrace_pixel</code> function of the <code>PathTracer</code> class, <code>ns_aa</code> camera rays are generated and traced through the scene. For each sample, a random offset is added to the pixel coordinates to generate a different camera ray. The generated ray is then passed to the <code>est_radiance_global_illumination</code> function, which traces the ray through the scene and computes the radiance at the intersection point. The computed radiance value is accumulated in a <code>Vector3D</code>. After all the samples are processed, the accumulated radiance values are averaged by dividing by the number of samples, and the resulting color is written to the sample buffer using the <code>update_pixel</code> function of the <code>SampleBuffer</code> class.
			
		<h3>Explain the triangle intersection algorithm you implemented in your own words.</h3>
			The triangle intersection algorithm uses MÃ¶ller-Trumbore to determine whether a given ray intersects with a triangle in 3D space, and if so, calculates the intersection point. First, it calculates two vectors from the vertices of the triangle, and a third vector from the origin of the ray to one of the triangle vertices. The algorithm then calculates the time of intersection, which allows us to determine the intersection point. 
			<br><br>
			These coordinates are used to interpolate the normal vector at the intersection point by taking a weighted sum of the normals at the three vertices of the triangle based on barycentric coordinate calculations. The intersection point and normal vector are then stored in an <code>Intersection</code> object. There are also checks to ensure that the determinant of the matrix is not negative, that the intersection point is within the valid range of the ray's parameters, and that barycentric coordinates have valid values. 
		
		<h3>Show images with normal shading for a few small <code>.dae</code> files.</h3>
			<img src="CBbunny.png" width="450">
			<br><br>
			<img src="CBcoil.png" width="450">
			<br><br>
			<img src="CBlucy.png" width="450">
			<br><br>

		<h3>Challenges</h3>
			In terms of challenges, we inititially struggled with converting from image space to camera space. We initially tried translating the (x, y) coordinates by (-0.5, -0.5). However, we didn't scale by 2, so this didn't work. We then tried removing the translation, and eventually realized (after consulting Ed) that we needed both translation and scaling. Also, when we started to look at the spec for part 2, we noticed tried rendering the cow. We noticed that we could see the different primitives and their edges showing on the surface despite the expected output being smooth curves. In order to fix this discrepancy, we changed the way we interpolated vertex normals with barycentric coordinates instead of Moller-Trumbore based on comments on Ed. 

	<h2>Part 2: Bounding Volume Hierarchy</h2>
		<h3>Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.</h3>
			A BVH is an acceleration structure used to improve rendering performance by reducing the number of ray-primitive intersection tests that need to be performed. It works by recursively subdividing the scene into bounding volumes that contain the primitives in the scene. The BVH is constructed by repeatedly subdividing the bounding volume into two smaller volumes, until a certain termination criterion is reached.
			<br><br>
			Once the BVH is constructed, the renderer can use it to quickly identify which bounding volumes are potentially intersected by a given ray, and test only the primitives that are contained in those volumes. This is done by traversing the BVH tree starting at the root, and recursively testing whether the ray intersects the bounding volume of each internal node. If the ray does not intersect the bounding volume, the entire subtree rooted at that node can be skipped. If the ray does intersect the bounding volume, the traversal continues down to the child nodes, until a leaf node containing a set of primitives is reached. At that point, the renderer tests each primitive in the leaf node to determine whether it intersects the ray.
		<h3>Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.</h3>
			<img src="maxplanck.png" width="450">
			<br><br>
			<img src="peter.png" width="450">
			<br><br>
		<h3>Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.</h3>
			<h4>Rendering runtime speedup with BVH</h4>
			<table>
				<tr>
					<th>file</th>
					<th>Initial time</th>
					<th>Time with BVH acceleration</th>
				</tr>
				<tr>
					<td>cow.dae</td>
					<td>5.6316s</td>
					<td>0.0385s</td>
				</tr>
				<tr>
					<td>beetle.dae</td>
					<td>8.3990s</td>
					<td>0.0370s</td>
				</tr>
				<tr>
					<td>teapot.dae</td>
					<td>2.1325s</td>
					<td>0.0415s</td>
				</tr>
				<tr>
					<td>maxplanck.dae</td>
					<td>75.1357s</td>
					<td>0.0460s</td>
				</tr>
			</table>
			<br><br>
			By using a BVH, the number of ray-primitive intersection tests can be greatly reduced, especially in scenes with many primitives. For example, in a scene with a large flat plane and many small objects scattered across it, most rays will not intersect the plane, and a BVH can be used to quickly skip the entire plane when testing those rays. Similarly, in a scene with many overlapping objects, a BVH can be used to quickly skip entire groups of objects that are occluded by other objects.
			<br><br>
			Challenges for this task included constructing the BVH in a manner that divides the mesh in three-dimensional space; this was addressed by applying the midpoint of each axis individually. Furthermore, finding a split point was difficult and took many iterations, but we were ultimately able to do so by merging the bounding boxes of each primtive and taking the centroid of the merged bounding box. For the acceleration portion, we were not recursively calling the left and right nodes, which caused feedback issues. This was addressed by calling the same function with the node updated to either left or right. Also, our initial <code>has_interaction</code> function was too complicated in the way it calculated if an interaction occurs, so the speedup was only around 2s, rather than the desired 0.0385s. In order to address this problem, we added broader checks that look at the bounding box and ray intersection to determine if interactions in the future might occur.

	<h2>Part 3: Direct Illumination</h2>
		<h3>Walk through both implementations of the direct lighting function.</h3>
			The <code>estimate_direct_lighting_hemisphere</code> function estimates the lighting from a given intersection coming directly from a light source by uniformly sampling a hemisphere. It first constructs a coordinate system at the hit point with the normal vector aligned with the Z direction. Then, it calculates the outgoing direction of the ray and generates a number of samples, for each of which it generates a new ray in a random direction in the hemisphere around the hit point. For each new ray, it checks if it intersects any object in the scene before it reaches a light source, and if so, it calculates the reflectance of the surface at the hit point and multiplies it by the emission of the intersected object, weighted by the cosine of the angle between the normal and the incoming ray. Finally, it averages the accumulated radiance across all the samples to estimate the direct lighting.
			<br><br>
			The <code>estimate_direct_lighting_importance</code> function estimates the lighting from a given intersection coming directly from a light source by importance sampling. This function is very similar to <code>estimate_direct_lighting_hemisphere</code>, but instead of uniformly sampling a hemisphere, it samples light sources in the scene. It first generates a number of samples. For area lights, this is equal to the number of light sources in the scene times a predefined constant <code>ns_area_light</code>, and for point lights, it equals one. For each sample, the program selects a random point on a light source and generates a ray from the hit point to that point. It then checks if the ray intersects any object in the scene before it reaches the light source. If so, the light from the light source does not reach the hit point directly, so we stop calculating the value to add. If the ray doesn't intersect anything, then the program calculates the reflectance of the surface at the hit point and multiplies it by the radiance of the light source, divided by the PDF value associated with the sample. Finally, it averages the accumulated radiance across all the samples to estimate the direct lighting.
			<br><br>
			A challenge we encountered with this part was that our initial implementation did not function as intended at all; it only rendered dark images. I (Zach) went back through our code and realized that, for importance sampling, we wanted the ray to not intersect anything in order for the light to transmit (in contrast to other functions, where we wanted it to intersect with a primitive in order to reflect light). The output was still much darker than intended, and the coloration was incorrect. Going through Ed and looking at the tips people had shared, I decided to revisit a lecture slide and realized that I was calculating <code>DiffuseBSDF::f</code> incorrectly. This fixed the color issue, but the image was still too dark. I attended office hours and spoke to Atsu. He pointed out that I didnât need to find the difference between the sample from <code>hemisphereSampler</code> and the hit point; I could just use the sample directly. After I made this change, the rendered images matched my expectations. I also forgot in my initial implementation to not add the calculated value when the calculated cosine was negative. I realized that this was something I should do from reading comments on Ed.

		<h3>Images rendered with both implementations of the direct lighting function.</h3>
			<h4>Image rendered with 1 light ray using light sampling</h4>
			<img src="bunny_64_32_LS.png" width="450">
			<h4>Image rendered with 4 light ray using light sampling</h4>
			<img src="bunny_64_32_LS2.png" width="450">
			<h4>Image rendered with 16 light ray using light sampling</h4>
			<img src="bunny_64_32_LS3.png" width="450">
			<h4>Image rendered with 64 light ray using light sampling</h4>
			<img src="bunny_64_32_LS4.png" width="450">
			<br><br>
			<h4>Image rendered with 1 light ray using uniform hemisphere sampling</h4>
			<img src="CBbunny_H_16_8.png" width="450">
			<h4>Image rendered with 4 light ray using uniform hemisphere sampling</h4>
			<img src="CBbunny_H_16_8 2.png" width="450">
			<h4>Image rendered with 16 light ray using uniform hemisphere sampling</h4>
			<img src="CBbunny_H_16_8 3.png" width="450">
			<h4>Image rendered with 64 light ray using uniform hemisphere sampling</h4>
			<img src="CBbunny_H_16_8 4.png" width="450">
			<br><br>
			Uniform hemisphere sampling samples points on a hemisphere centered around the surface normal, while lighting sampling only samples points on the surface of light sources in the scene. The advantage of uniform hemisphere sampling is its simplicity and ease of implementation, as well as its ability to capture indirect lighting effects. However, it can lead to a significant amount of noise and inaccuracy in areas of the scene that are not well lit. Lighting sampling, on the other hand, is a more accurate and efficient method for estimating direct lighting since it only samples from light sources, but it requires more complicated implementation and can potentially miss some light sources. In practice, a combination of both techniques is often used to achieve a balance between efficiency and accuracy.
			

	<h2>Part 4: Global Illumination</h2>

		<h3>Walk through your implementation of the indirect lighting function.</h3>
			For this part, we attempted to simulate global illumination, which includes direct and indirect lighting. This involves generating rays from the camera space into the scene and tracing them as they bounce around until they eventually hit a light source or reach a maximum number of bounces. This process generates a set of samples that we can use to estimate the transmitted light corresponding to each ray.
			<br><br>
			Specifically, we implemented a function called <code>at_least_one_bounce_radiance</code>, in which we first calculate the radiance from one bounce by calling <code>one_bounce_radiance</code>. Then, we call <code>sample_f</code>, a function that we wrote that calls other functions in order to get values for the incident light direction, the PDF, and the reflectance. If the cosine of the incident light direction (in object space) is positive, then we cast a new ray with the incident light direction (in world space). If the new ray intersects a primtive, and if we have not determined by Russian Roulette or by checking the ray depth value that we should not continue, then we add to the radiance. The value is calculated as the return value of a recursive call to this functioon with the new <code>Ray</code> and <code>Intersection</code>, multiplied by the reflectance and cosine values that we caclculated and divided by the PDF. If the bounce is not guaranteed, then it is also divided by the continuation probability.
			<br><br>
			Once we have computed the values for lighting with one or more bounces, we can add them to the zero-bounce values to get the final value for each ray. We then use these ray values to determine the color for each pixel, as discussed above. This enables us to render effects such as ambient occlusion or soft shadows. The quality of our image will depend on the number of samples we use for both direct and indirect lighting, as well as the complexity of our scene and the materials used on the objects. 
			<br><br>
			When I (Zach) first tried to implement <code>at_least_one_bounce</code>, I hadnât looked at the pseudocode in the lecture slides so I didnât understand the logic very well. I initially only added the direct illumination value for the first ray (ie, when <code>r.depth == max_ray_depth</code>). I also didnât realize initially that we had to have separate logic for âguaranteedâ bounces versus bounces that only proceeded when Russian Roulette determined that they should. When I realized this and implemented this more complicated conditional logic, I neglected to remove division by the cpdf for the âguaranteedâ bounces. We observed unexpectedly bright renderings and ended up going to the project party, where Ashley pointed out that flaw. When I adjusted the value for the âguaranteedâ bounces accordingly, the unexpected brightness no longer appeared in our renderings.

		<h3>Images rendered with global (direct and indirect) illumination at 1024 samples per pixel.</h3>
			<br><br>
			<img src="4-1-banana.png" width="450">
			<br><br>
			<img src="4-1-bunny.png" width="450">
			<br><br>
			<img src="4-1-spheres.png" width="450">
			<br><br>
		
		<h3>Direct vs. indirect illumination at 1024 samples per pixel</h3>

			<br><br>
			<img src="4-2-spheres-direct.png" width="450">
			<br><br>
			<img src="4-2-spheres-indirect.png" width="450">
			<br><br>
			The indirect and direct lighting scheme provide two images that are both incomplete. The one that is direct lighting has harsh shadows as the light bouncing off the walls and objects in the scene is not considering internal bouncing of the light before it reaches the camera. Vice versa is true for direct illumination. The two can be taken in combination to render an image that is accurate to real world lighting. 


		
		<h3>Bunny at 1024 samples per pixel and various levels of maximum ray depth</h3>
			<h4>Image rendered with maximum ray depth set to 0</h4>
			<img src="CBbunny_task4.png" width="450">
			<h4>Image rendered with maximum ray depth set to 1</h4>
			<img src="CBbunny_task4 2.png" width="450">
			<h4>Image rendered with maximum ray depth set to 2</h4>
			<img src="CBbunny_task4 3.png" width="450">
			<h4>Image rendered with maximum ray depth set to 3</h4>
			<img src="CBbunny_task4 4.png" width="450">
			<h4>Image rendered with maximum ray depth set to 100</h4>
			<img src="CBbunny_task4 5.png" width="450">
			<br><br>
			For CBbunny, by increasing max ray depth, we see that shadows get softer and the level of gradient change in lighting is less with each additional depth as the more bounces occur the brighter the shadows become as they include environmental secondary lighting. The effect of this is realized by the blue and red light tones being imposed onto the bunny as light that bounces from the walls is being included in the rendering. 


		<h3>Images with 4 light rays and various sample-per-pixel rates</h3>
			<h4>Image rendered with sample-per-pixel rate set to 1</h4>
			<img src="dragon_task4.png" width="450">
			<h4>Image rendered with sample-per-pixel rate set to 2</h4>
			<img src="dragon_task4 2.png" width="450">
			<h4>Image rendered with sample-per-pixel rate set to 4</h4>
			<img src="dragon_task4 3.png" width="450">
			<h4>Image rendered with sample-per-pixel rate set to 8</h4>
			<img src="dragon_task4 4.png" width="450">
			<h4>Image rendered with sample-per-pixel rate set to 16</h4>
			<img src="dragon_task4 5.png" width="450">
			<h4>Image rendered with sample-per-pixel rate set to 64</h4>
			<img src="dragon_task4 6.png" width="450">
			<h4>Image rendered with sample-per-pixel rate set to 1024</h4>
			<img src="dragon_task4 7.png" width="450">

	
	<h2>Part 5: Adaptive Sampling</h2>
		<h3>Intro to adaptive sampling</h3>
			Adaptive sampling is a technique to improve the efficiency of the path tracing algorithm. It is based on the idea that some regions of the image may require more samples than others to produce a noise-free result. Therefore, instead of using the same number of samples per pixel for the entire image, adaptive sampling adjusts the number of samples for each pixel based on an estimate of the variance of the pixel color. In other words, pixels that have high variance will receive more samples than pixels with low variance.
		<h3>Our implementation</h3>
		The implementation involves keeping track of the sum of the illuminance and the illuminance squared for all of the samples, which we can do by adding this value to the value of a variable with each iteration of our sampling loop. Using these sums, we can periodically calculate metrics that reflect how consistent our result has been and how many samples we have taken. When the results are highly consistent, we don't need as many samples, so we can break out of the sampling loop. Implementing this logic essentially involves counter variables, sum variables, and some algebra. We can then store the number of actual samples we took; this allows us to visualize the differences in sample sizes in different regions of the image, as seen below.
		<br></br>
		When testing my initial implementation for this part, I noticed that the rate image was all a solid color. I looked on Ed and saw people commenting that you should check that you are using appropriate variable types and doing your math correctly. It took me a few passes to spot it, but eventually I realized that I was neglecting to convert from variance to standard deviation. After correcting this and adjusting some of the numeric variableâs types, the rate image had variation, as expected.
		<br></br>

		<h3>Images demonstrating adaptive sampling</h3>
			<h4>Bunny</h4>
			<img src="bunny_task5.png" width="450">
			<img src="bunny_rate.png" width="450">

			<h4>Spheres</h4>
			<img src="CBspheres_lambertian.png" width="450">
			<img src="CBspheres_lambertian_rate.png" width="450">


	<h2>Collaboration statement</h2>
	We each took on different parts of the project, but made sure to stay updated on each other's progress and provide support whenever necessary. When we encountered difficulties or had questions, we utilized Ed to reach out to students and staff and get clarification in order to move forward. Overall, we thoroughly enjoyed working on this project together and learned a great deal about the importance of effective collaboration in achieving success. We believe that our ability to communicate openly and work together efficiently allowed us to produce a high-quality final product that we are both happy with. We also went to a project party, which helped us identify some ways to improve our code. 


    <h2>Link</h2>
    Link to write-up: <a href="https://zacharyzollman.github.io/cs184-proj-webpage/proj3-1/index.html">https://zacharyzollman.github.io/cs184-proj-webpage/proj3-1/index.html</a>
    </main>
	<script src="index.js"></script>
  </body>
</html>